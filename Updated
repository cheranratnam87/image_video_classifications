{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":9282942,"sourceType":"datasetVersion","datasetId":5618966}],"dockerImageVersionId":30762,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-08-30T16:25:16.805502Z","iopub.execute_input":"2024-08-30T16:25:16.805961Z","iopub.status.idle":"2024-08-30T16:25:16.813159Z","shell.execute_reply.started":"2024-08-30T16:25:16.805919Z","shell.execute_reply":"2024-08-30T16:25:16.811983Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pip install yt-dlp\n","metadata":{"execution":{"iopub.status.busy":"2024-08-30T16:40:43.664851Z","iopub.execute_input":"2024-08-30T16:40:43.665567Z","iopub.status.idle":"2024-08-30T16:41:03.478625Z","shell.execute_reply.started":"2024-08-30T16:40:43.665527Z","shell.execute_reply":"2024-08-30T16:41:03.477255Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pip install --upgrade pytube\n","metadata":{"execution":{"iopub.status.busy":"2024-08-30T16:39:38.104482Z","iopub.execute_input":"2024-08-30T16:39:38.105489Z","iopub.status.idle":"2024-08-30T16:39:53.368953Z","shell.execute_reply.started":"2024-08-30T16:39:38.105442Z","shell.execute_reply":"2024-08-30T16:39:53.367544Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pip install pytube opencv-python-headless transformers\n","metadata":{"execution":{"iopub.status.busy":"2024-08-30T16:38:02.372284Z","iopub.execute_input":"2024-08-30T16:38:02.372682Z","iopub.status.idle":"2024-08-30T16:38:18.092868Z","shell.execute_reply.started":"2024-08-30T16:38:02.372647Z","shell.execute_reply":"2024-08-30T16:38:18.09152Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pip install transformers torch torchvision\n","metadata":{"execution":{"iopub.status.busy":"2024-08-30T16:25:18.978236Z","iopub.execute_input":"2024-08-30T16:25:18.978653Z","iopub.status.idle":"2024-08-30T16:25:35.823904Z","shell.execute_reply.started":"2024-08-30T16:25:18.978613Z","shell.execute_reply":"2024-08-30T16:25:35.822585Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import AutoImageProcessor, AutoModelForImageClassification\nfrom PIL import Image\nimport requests\nfrom io import BytesIO\n\n# Load the image processor and model\nprocessor = AutoImageProcessor.from_pretrained(\"dima806/facial_emotions_image_detection\")\nmodel = AutoModelForImageClassification.from_pretrained(\"dima806/facial_emotions_image_detection\")\n","metadata":{"execution":{"iopub.status.busy":"2024-08-30T16:31:07.266485Z","iopub.execute_input":"2024-08-30T16:31:07.266921Z","iopub.status.idle":"2024-08-30T16:31:14.566248Z","shell.execute_reply.started":"2024-08-30T16:31:07.266881Z","shell.execute_reply":"2024-08-30T16:31:14.565251Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from PIL import Image\n\n# Load an image from a local path\nimg_path = '/kaggle/input/imaged/qwew.jpg'  # Replace with your local file path\nimg = Image.open(img_path)\n\n# Preprocess the image\ninputs = processor(images=img, return_tensors=\"pt\")\n","metadata":{"execution":{"iopub.status.busy":"2024-08-30T16:31:45.909051Z","iopub.execute_input":"2024-08-30T16:31:45.909496Z","iopub.status.idle":"2024-08-30T16:31:45.955277Z","shell.execute_reply.started":"2024-08-30T16:31:45.909452Z","shell.execute_reply":"2024-08-30T16:31:45.954302Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from PIL import Image\nfrom transformers import AutoImageProcessor, AutoModelForImageClassification\n\n# Load the image processor and model\nprocessor = AutoImageProcessor.from_pretrained(\"dima806/facial_emotions_image_detection\")\nmodel = AutoModelForImageClassification.from_pretrained(\"dima806/facial_emotions_image_detection\")\n\n# Load an image from a local path\nimg_path = '/kaggle/input/imaged/girl.jpg'  # Replace with your local file path\nimg = Image.open(img_path)\n\n# Preprocess the image\ninputs = processor(images=img, return_tensors=\"pt\")\n\n# Make predictions\noutputs = model(**inputs)\n\n# Get the predicted class\nlogits = outputs.logits\npredicted_class = logits.argmax(-1).item()\n\n# Access the model's labels\nlabels = model.config.id2label\npredicted_label = labels[predicted_class]\n\nprint(f'Predicted emotion: {predicted_label}')\n","metadata":{"execution":{"iopub.status.busy":"2024-08-30T16:34:01.350318Z","iopub.execute_input":"2024-08-30T16:34:01.350744Z","iopub.status.idle":"2024-08-30T16:34:01.820946Z","shell.execute_reply.started":"2024-08-30T16:34:01.350703Z","shell.execute_reply":"2024-08-30T16:34:01.819759Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from PIL import Image\nimport requests\nfrom io import BytesIO\nfrom transformers import AutoImageProcessor, AutoModelForImageClassification\n\n# Load the image processor and model\nprocessor = AutoImageProcessor.from_pretrained(\"dima806/facial_emotions_image_detection\")\nmodel = AutoModelForImageClassification.from_pretrained(\"dima806/facial_emotions_image_detection\")\n\n# Define the URL of the image\nurl = 'https://th.bing.com/th/id/OIP.CEQVIV0C_Dv-MYd6pB4E5gHaEK?rs=1&pid=ImgDetMain'  # Replace with your image URL\n\n# Fetch the image from the URL\nresponse = requests.get(url)\nimg = Image.open(BytesIO(response.content))\n\n# Preprocess the image\ninputs = processor(images=img, return_tensors=\"pt\")\n\n# Make predictions\noutputs = model(**inputs)\n\n# Get the predicted class\nlogits = outputs.logits\npredicted_class = logits.argmax(-1).item()\n\n# Access the model's labels\nlabels = model.config.id2label\npredicted_label = labels[predicted_class]\n\nprint(f'Predicted emotion: {predicted_label}')\n","metadata":{"execution":{"iopub.status.busy":"2024-08-30T17:00:15.351297Z","iopub.execute_input":"2024-08-30T17:00:15.352158Z","iopub.status.idle":"2024-08-30T17:00:15.928995Z","shell.execute_reply.started":"2024-08-30T17:00:15.352113Z","shell.execute_reply":"2024-08-30T17:00:15.926715Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import yt_dlp\nimport cv2\nimport numpy as np\nfrom PIL import Image\nfrom transformers import AutoImageProcessor, AutoModelForImageClassification\nimport os\n\n# Function to delete the existing video file\ndef cleanup(file_path):\n    if os.path.exists(file_path):\n        os.remove(file_path)\n\n# Function to download video using yt-dlp\ndef download_video(url, output_path='temp_video.mp4'):\n    cleanup(output_path)  # Ensure any existing video is deleted\n    ydl_opts = {\n        'format': 'mp4',\n        'outtmpl': output_path,\n    }\n    with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n        ydl.download([url])\n\n# Extract frames from video\ndef extract_frames_from_video(video_path, num_frames=10):\n    frames = []\n    cap = cv2.VideoCapture(video_path)\n    \n    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n    interval = total_frames // num_frames\n    \n    for i in range(num_frames):\n        cap.set(cv2.CAP_PROP_POS_FRAMES, i * interval)\n        ret, frame = cap.read()\n        if ret:\n            frames.append(frame)\n    \n    cap.release()\n    return frames\n\n# Predict emotion for each frame\ndef predict_emotion(frames, processor, model):\n    predictions = []\n    for frame in frames:\n        img = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n        inputs = processor(images=img, return_tensors=\"pt\")\n        outputs = model(**inputs)\n        logits = outputs.logits\n        predicted_class = logits.argmax(-1).item()\n        labels = model.config.id2label\n        predicted_label = labels[predicted_class]\n        predictions.append(predicted_label)\n    return predictions\n\n# Main function\ndef analyze_video_emotion(youtube_url):\n    # Load model\n    processor = AutoImageProcessor.from_pretrained(\"dima806/facial_emotions_image_detection\")\n    model = AutoModelForImageClassification.from_pretrained(\"dima806/facial_emotions_image_detection\")\n\n    # Download video\n    download_video(youtube_url)\n    \n    # Extract frames from the downloaded video\n    frames = extract_frames_from_video('temp_video.mp4', num_frames=10)\n    \n    # Predict emotion for each frame\n    predictions = predict_emotion(frames, processor, model)\n    \n    # Calculate the overall emotion\n    overall_emotion = max(set(predictions), key=predictions.count)\n    return overall_emotion\n\n# Example usage\nyoutube_url = 'https://www.youtube.com/watch?v=SpvC6jlqldk' #GF\noverall_emotion = analyze_video_emotion(youtube_url)\nprint(f'Overall emotion: {overall_emotion}')\n\n# Test another video\nyoutube_url_new = 'https://www.youtube.com/watch?v=vBk5y0iu3yI'#SC\noverall_emotion_new = analyze_video_emotion(youtube_url_new)\nprint(f'Overall emotion for new video: {overall_emotion_new}')\n","metadata":{"execution":{"iopub.status.busy":"2024-08-30T17:01:01.637434Z","iopub.execute_input":"2024-08-30T17:01:01.637896Z","iopub.status.idle":"2024-08-30T17:01:17.201365Z","shell.execute_reply.started":"2024-08-30T17:01:01.637855Z","shell.execute_reply":"2024-08-30T17:01:17.200017Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#K.H SPEECH\n\nyoutube_url_new3 = 'https://www.youtube.com/watch?v=1aZLL63wlTU'\noverall_emotion_new = analyze_video_emotion(youtube_url_new3)\nprint(f'Overall emotion for new video: {overall_emotion_new}')\n","metadata":{"execution":{"iopub.status.busy":"2024-08-30T16:53:28.036754Z","iopub.execute_input":"2024-08-30T16:53:28.03727Z","iopub.status.idle":"2024-08-30T16:53:36.772046Z","shell.execute_reply.started":"2024-08-30T16:53:28.037224Z","shell.execute_reply":"2024-08-30T16:53:36.770861Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#JDV SPEECH\n\nyoutube_url_new3 = 'https://www.youtube.com/watch?v=cYzXBA7zucI'\noverall_emotion_new = analyze_video_emotion(youtube_url_new3)\nprint(f'Overall emotion for new video: {overall_emotion_new}')\n","metadata":{"execution":{"iopub.status.busy":"2024-08-30T16:54:01.822115Z","iopub.execute_input":"2024-08-30T16:54:01.823002Z","iopub.status.idle":"2024-08-30T16:54:10.919849Z","shell.execute_reply.started":"2024-08-30T16:54:01.822956Z","shell.execute_reply":"2024-08-30T16:54:10.918701Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#T SPEECH\n\nyoutube_url_new3 = 'https://www.youtube.com/watch?v=YG37yFh7SzM'\noverall_emotion_new = analyze_video_emotion(youtube_url_new3)\nprint(f'Overall emotion for new video: {overall_emotion_new}')\n","metadata":{"execution":{"iopub.status.busy":"2024-08-30T16:56:09.444889Z","iopub.execute_input":"2024-08-30T16:56:09.44534Z","iopub.status.idle":"2024-08-30T16:56:19.139115Z","shell.execute_reply.started":"2024-08-30T16:56:09.445302Z","shell.execute_reply":"2024-08-30T16:56:19.137893Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#WALZ SPEECH\n\nyoutube_url_new3 = 'https://www.youtube.com/watch?v=Mf0u5MJEjhw'\noverall_emotion_new = analyze_video_emotion(youtube_url_new3)\nprint(f'Overall emotion for new video: {overall_emotion_new}')\n","metadata":{"execution":{"iopub.status.busy":"2024-08-30T16:57:20.63012Z","iopub.execute_input":"2024-08-30T16:57:20.630599Z","iopub.status.idle":"2024-08-30T16:57:28.458099Z","shell.execute_reply.started":"2024-08-30T16:57:20.630555Z","shell.execute_reply":"2024-08-30T16:57:28.456906Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}